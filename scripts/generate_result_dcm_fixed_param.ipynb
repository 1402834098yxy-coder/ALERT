{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Clustering Results with Fixed Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "datasets = ['nytimes','enron']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Clustering threshold $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(f\"Different Clustering threshold theta for {dataset}\")\n",
    "    with open(f'{PROJECT_ROOT}/log/{dataset}_add_del_new_train_1_0_0.5-0.5_None_1_0.pkl','rb') as f:\n",
    "        matrix1 = pickle.load(f)\n",
    "    with open(f'{PROJECT_ROOT}/log/{dataset}_add_del_new_test_1_0_0.5-0.5_None_1_0.pkl','rb') as f:\n",
    "        matrix2 = pickle.load(f)\n",
    "    matrix1 = np.sum(matrix1, axis=0)\n",
    "    matrix2 = np.sum(matrix2, axis=0)\n",
    "    #create a plot to draw a histogram\n",
    "    whole_acc_list = []\n",
    "    group_threshold_list = [5,10,20,50,100,200,500]\n",
    "    # deletion rate =10%\n",
    "    for m in range(7):\n",
    "        temp_matrix1 = matrix1[:1000]\n",
    "        temp_matrix2 = matrix2[:1000]\n",
    "        indices1 = np.arange(3000)\n",
    "        indices2 = np.arange(3000)\n",
    "        correlation = np.corrcoef(temp_matrix1.flatten(), temp_matrix2.flatten())[0, 1]\n",
    "        # print(\"correlation score is \", correlation)\n",
    "        # extract their diag matrix\n",
    "        diag1 = np.diag(temp_matrix1)\n",
    "        diag2 = np.diag(temp_matrix2)\n",
    "        # sort the diag1 and 2, the index should be the same\n",
    "        sorted_indices1 = np.argsort(-diag1)\n",
    "        sorted_indices2 = np.argsort(-diag2)\n",
    "        \n",
    "        group_threshold = group_threshold_list[m]\n",
    "        \n",
    "        group_num = int(len(sorted_indices1) / (group_threshold))\n",
    "        acc_list = []\n",
    "        # print(\"sorted_indices1\",sorted_indices1)\n",
    "        # print(\"sorted_indices2\",sorted_indices2)\n",
    "        \n",
    "        for i in range(group_num):\n",
    "            acc_count  = 0 \n",
    "            group_indices1 = sorted_indices1[i*group_threshold:(i+1)*group_threshold]\n",
    "            group_indices2 = sorted_indices2[i*group_threshold:(i+1)*group_threshold]\n",
    "            # print(group_indices1)\n",
    "            # print(group_indices2)\n",
    "            for j in range(group_threshold):\n",
    "                # if group_indices1[j] is contained in group_indices2, then acc_count + 1\n",
    "                if group_indices1[j] in group_indices2:\n",
    "                    acc_count += 1\n",
    "            match_accuracy = acc_count / group_threshold\n",
    "            # print(f'Match accuracy for group {i+1}: {match_accuracy}')\n",
    "            acc_list.append(match_accuracy)\n",
    "        whole_acc = sum(acc_list) / len(acc_list)\n",
    "        print(\"group threshold\\t\",group_threshold,\"\\tWhole match accuracy\\t\",whole_acc)\n",
    "        whole_acc_list.append(whole_acc)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different keyword number $\\vartheta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "whole_acc_list = []\n",
    "keywords_list = [100,200,300,500,1000,2000,3000]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Different keyword number vartheta for {dataset}\")\n",
    "    with open(f'{PROJECT_ROOT}/log/{dataset}_add_del_new_train_1_0_0.5-0.5_None_1_0.pkl','rb') as f:\n",
    "        matrix1 = pickle.load(f)\n",
    "    with open(f'{PROJECT_ROOT}/log/{dataset}_add_del_new_test_1_0_0.5-0.5_None_1_0.pkl','rb') as f:\n",
    "        matrix2 = pickle.load(f)\n",
    "    matrix1 = np.sum(matrix1, axis=0)\n",
    "    matrix2 = np.sum(matrix2, axis=0)\n",
    "    for m in range(7):\n",
    "        temp_matrix1 = matrix1[:keywords_list[m]]\n",
    "        temp_matrix2 = matrix2[:keywords_list[m]]\n",
    "        indices1 = np.arange(3000)\n",
    "        indices2 = np.arange(3000)\n",
    "        correlation = np.corrcoef(temp_matrix1.flatten(), temp_matrix2.flatten())[0, 1]\n",
    "        # print(\"correlation score is \", correlation)\n",
    "        # extract their diag matrix\n",
    "        diag1 = np.diag(temp_matrix1)\n",
    "        diag2 = np.diag(temp_matrix2)\n",
    "        # sort the diag1 and 2, the index should be the same\n",
    "        sorted_indices1 = np.argsort(-diag1)\n",
    "        sorted_indices2 = np.argsort(-diag2)\n",
    "        \n",
    "        group_threshold = 50\n",
    "        group_num = int(len(sorted_indices1) / (group_threshold))\n",
    "        acc_list = []\n",
    "        # print(\"sorted_indices1\",sorted_indices1)\n",
    "        # print(\"sorted_indices2\",sorted_indices2)\n",
    "        \n",
    "        for i in range(group_num):\n",
    "            acc_count  = 0 \n",
    "            group_indices1 = sorted_indices1[i*group_threshold:(i+1)*group_threshold]\n",
    "            group_indices2 = sorted_indices2[i*group_threshold:(i+1)*group_threshold]\n",
    "            # print(group_indices1)\n",
    "            # print(group_indices2)\n",
    "            for j in range(group_threshold):\n",
    "                # if group_indices1[j] is contained in group_indices2, then acc_count + 1\n",
    "                if group_indices1[j] in group_indices2:\n",
    "                    acc_count += 1\n",
    "            match_accuracy = acc_count / group_threshold\n",
    "            # print(f'Match accuracy for group {i+1}: {match_accuracy}')\n",
    "            acc_list.append(match_accuracy)\n",
    "        whole_acc = sum(acc_list) / len(acc_list)\n",
    "        # print('group threhold',group_threshold)\n",
    "        print(\"keywords number\\t\",keywords_list[m],\"\\tWhole match accuracy\\t\",whole_acc)\n",
    "        # print(f'Whole match accuracy: {whole_acc}')\n",
    "        whole_acc_list.append(whole_acc)\n",
    "        #dump the whole acc into a json file, just add at the end of the current json file\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different sampling rate $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_acc_list = []\n",
    "keywords_list = [3000]\n",
    "candidates = [\"0.5-0.5\", \"0.3-0.7\", \"0.2-0.8\", \"0.1-0.9\",\"0.05-0.95\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"Different sampling rate alpha for {dataset}\")\n",
    "    for m in range(5):\n",
    "        train_file = f\"{PROJECT_ROOT}/log/{dataset}_add_del_new_train_1_0_\" + candidates[m] + \"_None_1_0.pkl\"\n",
    "        test_file = f\"{PROJECT_ROOT}/log/{dataset}_add_del_new_test_1_0_\" + candidates[m] + \"_None_1_0.pkl\"\n",
    "        # print(\"train_file\",train_file)\n",
    "        with open(train_file,'rb') as f:\n",
    "            matrix1 = pickle.load(f)\n",
    "        with open(test_file,'rb') as f:\n",
    "            matrix2 = pickle.load(f)\n",
    "        matrix1 = np.sum(matrix1, axis=0)\n",
    "        matrix2 = np.sum(matrix2, axis=0)\n",
    "        # matrix1 = matrix1[29]\n",
    "        # matrix2 = matrix2[29]\n",
    "        \n",
    "        temp_matrix1 = matrix1[:1000]\n",
    "        temp_matrix2 = matrix2[:1000]\n",
    "        # print(\"keywords number\",keywords_list[m])\n",
    "        indices1 = np.arange(3000)\n",
    "        indices2 = np.arange(3000)\n",
    "        correlation = np.corrcoef(temp_matrix1.flatten(), temp_matrix2.flatten())[0, 1]\n",
    "        # print(\"correlation score is \", correlation)\n",
    "        # extract their diag matrix\n",
    "        diag1 = np.diag(temp_matrix1)\n",
    "        diag2 = np.diag(temp_matrix2)\n",
    "        # sort the diag1 and 2, the index should be the same\n",
    "        sorted_indices1 = np.argsort(-diag1)\n",
    "        sorted_indices2 = np.argsort(-diag2)\n",
    "        \n",
    "        group_threshold = 50\n",
    "        group_num = int(len(sorted_indices1) / (group_threshold))\n",
    "        acc_list = []\n",
    "        # print(\"sorted_indices1\",sorted_indices1)\n",
    "        # print(\"sorted_indices2\",sorted_indices2)\n",
    "        \n",
    "        for i in range(group_num):\n",
    "            acc_count  = 0 \n",
    "            group_indices1 = sorted_indices1[i*group_threshold:(i+1)*group_threshold]\n",
    "            group_indices2 = sorted_indices2[i*group_threshold:(i+1)*group_threshold]\n",
    "            # print(group_indices1)\n",
    "            # print(group_indices2)\n",
    "            for j in range(group_threshold):\n",
    "                # if group_indices1[j] is contained in group_indices2, then acc_count + 1\n",
    "                if group_indices1[j] in group_indices2:\n",
    "                    acc_count += 1\n",
    "            match_accuracy = acc_count / group_threshold\n",
    "            acc_list.append(match_accuracy)\n",
    "        whole_acc = sum(acc_list) / len(acc_list)\n",
    "        sampling_rate_pct = float(candidates[m].split('-')[0]) * 100\n",
    "        print(\"sampling rate\\t\",f\"{sampling_rate_pct}%\",\"\\tWhole match accuracy\\t\",whole_acc)\n",
    "        whole_acc_list.append(whole_acc)\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sse2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
